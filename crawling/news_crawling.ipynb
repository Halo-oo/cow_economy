{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295b7f11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sid1 101\n",
      "sid2 259\n",
      "date 2023-03-11 10:43:10.031662+09:00\n",
      "page 1\n",
      "page 2\n",
      "2023-03-12 10:43:10.031662+09:00\n",
      "전체 길이 18\n",
      "소요 시간 3.55265474319458초\n",
      "결과 데이터:                  now()\n",
      "0 2023-03-13 10:43:13\n",
      "DB연결 성공\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import datetime as dt\n",
    "from pytz import timezone\n",
    "\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from sqlalchemy import create_engine, text\n",
    "pymysql.install_as_MySQLdb()\n",
    "import MySQLdb\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# 시작 시간\n",
    "start = time.time()\n",
    "\n",
    "# DB에 저장할 Dataframe 설정\n",
    "column_name = [\"article_category\", \"article_regtime\", \"article_editor\", \"article_press\", \"article_title\",\n",
    "               \"article_thumbnail\", \"article_content\", \"article_url\", \"article_hits\"]\n",
    "\n",
    "# category\n",
    "sid1 = (101,) # 대분류\n",
    "# sid2 = {'금융': 259, '증권': 258, '산업/재계': 261, '중기/벤처': 771, '부동산': 260, '글로벌 경제': 262, '생활경제': 310, '경제 일반': 263} # 소분류\n",
    "sid2 = {'금융': 259} # 테스트용 소분류\n",
    "\n",
    "# date \n",
    "end_date = dt.datetime.now(timezone('Asia/Seoul')) - dt.timedelta(days=2)  # 어제\n",
    "start_date = end_date\n",
    "# start_date = end_date - dt.timedelta(days=1)   # 하루 전 날짜\n",
    "\n",
    "# result\n",
    "results = []\n",
    "\n",
    "# press\n",
    "press_list = ['매일경제', '머니투데이', '비즈워치', '서울경제', '아시아경제', '이데일리', '조선비즈', '조세일보', '파이낸셜뉴스', '한국경제', '헤럴드경제',\n",
    "              '경향신문', '국민일보', '동아일보', '문화일보', '서울신문', '세계일보', '조선일보', '중앙일보', '한계레', '한국일보']\n",
    "\n",
    "# separator\n",
    "separator = \"@@div\"\n",
    "separator_image = \"@@divimg\"\n",
    "separator_image_desc = \"@@divimgdesc\"\n",
    "\n",
    "# debug\n",
    "debug = True\n",
    "\n",
    "# 대분류로 반복\n",
    "for main in sid1:\n",
    "    if debug: print(\"sid1\", main)\n",
    "\n",
    "    # 소분류로 반복\n",
    "    for subkey, subval in sid2.items():\n",
    "        if debug: print(\"sid2\", subval)\n",
    "\n",
    "        # 날짜로 반복\n",
    "        s_date = start_date\n",
    "        while s_date <= end_date:\n",
    "            if debug: print(\"date\", s_date)\n",
    "\n",
    "            # 페이지로 반복\n",
    "            page = 1\n",
    "            max_page = 1\n",
    "\n",
    "            while page <= 2:\n",
    "                if debug: print(\"page\", page)\n",
    "                response = urlopen(\n",
    "                    f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2={subval}&sid1={main}&date={s_date.strftime(\"%Y%m%d\")}&page={page}')\n",
    "                soup = BeautifulSoup(response, \"html.parser\")\n",
    "               \n",
    "                # 페이지 수 구하기\n",
    "                if page % 10 == 1:\n",
    "                    page_a_list = soup.find(\"div\", {\"class\": \"paging\"}).find_all(\"a\")\n",
    "                    if page_a_list:\n",
    "                        last_value = page_a_list[-1].get_text()\n",
    "                        if last_value == '다음':\n",
    "                            max_page += 10\n",
    "                        elif last_value.isdigit():\n",
    "                            max_page = int(last_value)\n",
    "\n",
    "                # 현재 페이지 리스트에 있는 기사의 링크 가져오기\n",
    "                value = soup.find_all(\"div\", {\"class\": \"newsflash_body\"})\n",
    "\n",
    "                for i in value:\n",
    "                    links = i.find_all(\"dl\", class_=False)\n",
    "\n",
    "                    # 링크를 반복하며 세부 기사 페이지에서 내용 가져오기\n",
    "                    for link in links:\n",
    "\n",
    "                        li = link.find(\"dt\", class_=False).find(\"a\").attrs[\"href\"]  # 뉴스 상세 조회 링크\n",
    "                        press_name = link.find(\"span\", {\"class\": \"writing\"}).get_text()\n",
    "\n",
    "                        if li is None or (press_name not in press_list):\n",
    "                            continue\n",
    "\n",
    "                        detail_response = urlopen(li)\n",
    "                        detail_soup = BeautifulSoup(detail_response, \"html.parser\")\n",
    "\n",
    "                        header = detail_soup.find(\"div\", {\"class\": \"media_end_head\"})\n",
    "\n",
    "                        detail = {}\n",
    "                        # header가 없으면 continue\n",
    "                        if header is None:\n",
    "                            continue\n",
    "\n",
    "                        # 기사 카테고리 (article_category)\n",
    "                        detail[\"article_category\"] = subkey\n",
    "\n",
    "                        # 발행일시 (article_regtime)\n",
    "                        date = header.find(\"span\", {\"class\": \"media_end_head_info_datestamp_time\"})['data-date-time']\n",
    "                        detail[\"article_regtime\"] = date\n",
    "\n",
    "                        # 기자 (article_editor)\n",
    "                        reporter = header.find(\"em\", {\"class\": \"media_end_head_journalist_name\"})\n",
    "                        detail[\"article_editor\"] = \"\"\n",
    "                        if reporter is not None:\n",
    "                            detail[\"article_editor\"] = reporter.get_text()\n",
    "                       \n",
    "                        # 언론사 (article_press)\n",
    "                        detail[\"article_press\"] = press_name\n",
    "\n",
    "                        # 기사 제목 (article_title)\n",
    "                        detail[\"article_title\"] = header.find(\"h2\").find(\"span\").get_text()\n",
    "\n",
    "                        # 기사 내용 (article_content) - 소제목, 이미지, 이미지 설명\n",
    "                        contents = detail_soup.find(\"div\", {\"class\": \"_article_content\"})\n",
    "                        content = \"\"\n",
    "\n",
    "                        # print(\"result_before\", contents)\n",
    "                        br_list = contents.find_all(\"br\")\n",
    "\n",
    "                        for br in br_list:\n",
    "                            br.replace_with(\"@@br\")\n",
    "\n",
    "                        td_list = contents.find_all(\"td\")\n",
    "                        for td in td_list:\n",
    "                          if(td.find_all(\"table\")):\n",
    "                            continue\n",
    "                          img_desc = td.get_text()\n",
    "                          if len(img_desc.strip())>0:\n",
    "                            td.replace_with(separator_image_desc + td.get_text() + separator)\n",
    "\n",
    "                        img_list = contents.find_all(\"img\")\n",
    "                        if len(img_list) != 0 :\n",
    "                            detail[\"article_thumbnail\"] = img_list[0].get(\"data-src\")\n",
    "                        \n",
    "                        for img in img_list:\n",
    "                            img.replace_with(separator_image + img.get(\"data-src\") + separator)\n",
    "\n",
    "                        em_list = contents.find_all(\"em\")\n",
    "                        for em in em_list:\n",
    "                            em.replace_with(separator_image_desc + em.get_text() + separator)\n",
    "\n",
    "                        strong_list = contents.find_all(\"strong\")\n",
    "                        for strong in strong_list:\n",
    "                            strong.replace_with(\"@@strong\" + strong.get_text() + \"@@strong\")\n",
    "\n",
    "                        b_list = contents.find_all(\"b\")\n",
    "                        for b in b_list:\n",
    "                            b.replace_with(\"@@b\" + b.get_text() + \"@@b\")\n",
    "\n",
    "#                         print(\"본문\\n\", contents.get_text().strip())\n",
    "\n",
    "                        detail[\"article_content\"] = contents.get_text().strip()\n",
    "\n",
    "                        # 원본 링크 (article_url)\n",
    "                        original = header.find(\"a\", {\"class\": \"media_end_head_origin_link\"})['href']\n",
    "                        detail[\"article_url\"] = original\n",
    "\n",
    "#                         detail[\"article_thumbnail\"] = \"\"\n",
    "#                         # 썸네일 (article_thumbnail)\n",
    "#                         if detail_soup.find(\"img\", {\"id\": \"img1\"}) is not None:\n",
    "#                             detail[\"article_thumbnail\"] = detail_soup.find(\"img\", {\"id\": \"img1\"})[\"data-src\"]\n",
    "#                             print(detail_soup.find(\"img\", {\"id\": \"img1\"})[\"data-src\"])\n",
    "                        # 조회수\n",
    "                        detail[\"article_hits\"] = 0\n",
    "\n",
    "                        results.append(detail)\n",
    "#                         print(detail, \"\\n\")\n",
    "                page += 1\n",
    "            \n",
    "            # 하루 더해서 다음날로 넘어가기\n",
    "            s_date += dt.timedelta(days=1)\n",
    "            print(s_date)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"전체 길이\", len(results))\n",
    "\n",
    "# 뉴스 데이터 Dataframe 설정\n",
    "news_df = pd.DataFrame(results, columns=column_name)\n",
    "\n",
    "# 종료 시간\n",
    "end = time.time()\n",
    "\n",
    "print(f'소요 시간 {end - start}초')\n",
    "\n",
    "# DB로 저장\n",
    "db_connection_str = 'mysql+pymysql://root:ssafy@localhost:3306/ssafy_cow_db'\n",
    "db_connection = create_engine(db_connection_str)\n",
    "# conn = db_connection.connect()\n",
    "# conn.close()\n",
    "sql = \"SELECT now() FROM dual\"\n",
    "df = pd.DataFrame(db_connection.connect().execute(text(sql)))\n",
    "print(\"결과 데이터: \", df)     # 전체 rows\n",
    "print(\"DB연결 성공\")\n",
    "\n",
    "# news db에 넣기\n",
    "# news_df.to_sql(name='article', con=db_connection, if_exists='append',index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "전체 길이 2762\n",
    "소요 시간 814.393415927887초\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4c975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
