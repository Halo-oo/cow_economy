{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "295b7f11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import datetime as dt\n",
    "from pytz import timezone\n",
    "\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from sqlalchemy import create_engine, text\n",
    "pymysql.install_as_MySQLdb()\n",
    "import MySQLdb\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "import os\n",
    "\n",
    "import hdfs_newsdata as newsdata\n",
    "\n",
    "def readArticleTime():\n",
    "    f = open(\"/home/j8a509/crawling/article_time.txt\", 'r')\n",
    "    last_article_time = f.readlines()\n",
    "    f.close()\n",
    "    return last_article_time\n",
    "\n",
    "def writeArticleTime(dates) :\n",
    "    f = open(\"/home/j8a509/crawling/article_time.txt\", 'w')\n",
    "    print(dates)\n",
    "    f.write(dates)\n",
    "    f.close()\n",
    "\n",
    "def writeLastArticleId(article) :\n",
    "    f = open(\"/home/j8a509/crawling/last_article.txt\", 'w')\n",
    "    print(article)\n",
    "    f.write(article)\n",
    "    f.close()\n",
    "    \n",
    "# 시작 시간\n",
    "start = time.time()\n",
    "\n",
    "# DB에 저장할 Dataframe 설정\n",
    "column_name = [\"article_id\",\"article_category\", \"article_regtime\", \"article_editor\", \"article_press\", \"article_title\",\n",
    "               \"article_thumbnail\", \"article_content\", \"article_url\", \"article_hits\"]\n",
    "\n",
    "# category\n",
    "sid1 = (101,) # 대분류\n",
    "sid2 = {'금융': 259, '증권': 258, '산업/재계': 261, '중기/벤처': 771, '부동산': 260, '글로벌 경제': 262, '생활경제': 310, '경제 일반': 263} # 소분류\n",
    "\n",
    "# date \n",
    "end_date = dt.datetime.now(timezone('Asia/Seoul'))\n",
    "start_date = end_date\n",
    "# result\n",
    "results = []\n",
    "\n",
    "# press\n",
    "press_list = ['매일경제', '머니투데이', '비즈워치', '서울경제', '아시아경제', '이데일리', '조선비즈', '조세일보', '파이낸셜뉴스', '한국경제', '헤럴드경제',\n",
    "              '경향신문', '국민일보', '동아일보', '문화일보', '서울신문', '세계일보', '조선일보', '중앙일보', '한계레', '한국일보']\n",
    "\n",
    "# separator\n",
    "separator_image = \"@@img\"\n",
    "separator_image_end = \"@@endimg\"\n",
    "separator_image_desc = \"@@imgdesc\"\n",
    "separator_image_desc_end = \"@@endimgdesc\"\n",
    "separator_strong = \"@@strong\"\n",
    "separator_strong_end = \"@@endstrong\"\n",
    "\n",
    "# debug\n",
    "debug = True\n",
    "\n",
    "# DB 연결\n",
    "db_connection_str = 'mysql+pymysql://root:ssafy@j8a509.p.ssafy.io:3306/ssafy_cow_db'\n",
    "db_connection = create_engine(db_connection_str)\n",
    "\n",
    "# 데이터 조회 \n",
    "sql = \"select max(article_id) as max_id from article\"\n",
    "\n",
    "df = pd.DataFrame(db_connection.connect().execute(text(sql)))\n",
    "last_article_id = 0\n",
    "\n",
    "if df.iloc[0]['max_id'] is not None:\n",
    "    last_article_id = df.iloc[0]['max_id']\n",
    "\n",
    "# 마지막으로 저장된 기사 아이디 저장\n",
    "writeLastArticleId(str(last_article_id))\n",
    "\n",
    "# 대분류로 반복\n",
    "for main in sid1:\n",
    "    if debug: print(\"sid1\", main)\n",
    "    dates = readArticleTime()\n",
    "    write_date = \"\"\n",
    "    # 소분류로 반복\n",
    "    for subkey, subval in sid2.items():\n",
    "        index = 0\n",
    "        if debug: print(\"sid2\", subval)\n",
    "        # 소분류별 마지막 기사 시간\n",
    "        print(subkey,\" 마지막 기사 시간: \", dates[list(sid2).index(subkey)])\n",
    "        last_date = dates[list(sid2).index(subkey)]\n",
    "        # 날짜로 반복\n",
    "        s_date = start_date\n",
    "        while s_date <= end_date:\n",
    "            # 페이지로 반복\n",
    "            page = 1\n",
    "            max_page = 1\n",
    "\n",
    "            while page <= max_page:\n",
    "                if debug: print(\"page\", page)\n",
    "                response = urlopen(\n",
    "                    f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2={subval}&sid1={main}&date={s_date.strftime(\"%Y%m%d\")}&page={page}')\n",
    "                soup = BeautifulSoup(response, \"html.parser\")\n",
    "               \n",
    "                # 페이지 수 구하기\n",
    "                if page % 10 == 1:\n",
    "                    page_a_list = soup.find(\"div\", {\"class\": \"paging\"}).find_all(\"a\")\n",
    "                    if page_a_list:\n",
    "                        last_value = page_a_list[-1].get_text()\n",
    "                        if last_value == '다음':\n",
    "                            max_page += 10\n",
    "                        elif last_value.isdigit():\n",
    "                            max_page = int(last_value)\n",
    "\n",
    "                # 현재 페이지 리스트에 있는 기사의 링크 가져오기\n",
    "                value = soup.find_all(\"div\", {\"class\": \"newsflash_body\"})\n",
    "\n",
    "                for i in value:\n",
    "                    links = i.find_all(\"dl\", class_=False)\n",
    "                    \n",
    "                    # 링크를 반복하며 세부 기사 페이지에서 내용 가져오기\n",
    "                    for link in links:\n",
    "                        \n",
    "                        li = link.find(\"dt\", class_=False).find(\"a\").attrs[\"href\"]  # 뉴스 상세 조회 링크\n",
    "                        press_name = link.find(\"span\", {\"class\": \"writing\"}).get_text()\n",
    "\n",
    "                        if li is None or (press_name not in press_list):\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            detail_response = urlopen(li)\n",
    "                            detail_soup = BeautifulSoup(detail_response, \"html.parser\")\n",
    "\n",
    "                            header = detail_soup.find(\"div\", {\"class\": \"media_end_head\"})\n",
    "\n",
    "                            # header가 없으면 continue\n",
    "                            if header is None:\n",
    "                                continue\n",
    "\n",
    "                            article_time = header.find(\"span\", {\"class\": \"media_end_head_info_datestamp_time\"})['data-date-time']\n",
    "\n",
    "                            # 마지막 기사 등록일자 이후 기사만 크롤링\n",
    "                            if(article_time<=last_date):\n",
    "                                break\n",
    "\n",
    "                            detail = {}\n",
    "\n",
    "                            # 기사 카테고리 (article_category)\n",
    "                            detail[\"article_category\"] = subkey\n",
    "\n",
    "                            # 발행일시 (article_regtime)\n",
    "                            date = header.find(\"span\", {\"class\": \"media_end_head_info_datestamp_time\"})['data-date-time']\n",
    "                            detail[\"article_regtime\"] = date\n",
    "\n",
    "                            # 기자 (article_editor)\n",
    "                            reporter = header.find(\"em\", {\"class\": \"media_end_head_journalist_name\"})\n",
    "                            detail[\"article_editor\"] = \"\"\n",
    "                            if reporter is not None:\n",
    "                                detail[\"article_editor\"] = reporter.get_text()\n",
    "\n",
    "                            # 언론사 (article_press)\n",
    "                            detail[\"article_press\"] = press_name\n",
    "\n",
    "                            # 기사 제목 (article_title)\n",
    "                            detail[\"article_title\"] = header.find(\"h2\").find(\"span\").get_text()\n",
    "\n",
    "                            # 기사 내용 (article_content) - 소제목, 이미지, 이미지 설명\n",
    "                            contents = detail_soup.find(\"div\", {\"class\": \"_article_content\"})\n",
    "                            content = \"\"\n",
    "\n",
    "                            # print(\"result_before\", contents)\n",
    "                            br_list = contents.find_all(\"br\")\n",
    "\n",
    "                            for br in br_list:\n",
    "                                br.replace_with(\"@@br\")\n",
    "\n",
    "                            td_list = contents.find_all(\"td\")\n",
    "                            for td in td_list:\n",
    "                              if(td.find_all(\"table\")):\n",
    "                                continue\n",
    "                              img_desc = td.get_text()\n",
    "                              if len(img_desc.strip())>0:\n",
    "                                td.replace_with(separator_image_desc + td.get_text() + separator_image_desc_end)\n",
    "\n",
    "                            img_list = contents.find_all(\"img\")\n",
    "                            if len(img_list) != 0 :\n",
    "                                detail[\"article_thumbnail\"] = img_list[0].get(\"data-src\")\n",
    "\n",
    "                            for img in img_list:\n",
    "                                img.replace_with(separator_image + img.get(\"data-src\") + separator_image_end)\n",
    "\n",
    "                            em_list = contents.find_all(\"em\")\n",
    "                            for em in em_list:\n",
    "                                em.replace_with(separator_image_desc + em.get_text() + separator_image_desc_end)\n",
    "\n",
    "                            strong_list = contents.find_all(\"strong\")\n",
    "                            for strong in strong_list:\n",
    "                                strong.replace_with(separator_strong + strong.get_text() + separator_strong_end)\n",
    "\n",
    "                            b_list = contents.find_all(\"b\")\n",
    "                            for b in b_list:\n",
    "                                b.replace_with(separator_strong + b.get_text() + separator_strong_end)\n",
    "\n",
    "                            tmp_content = contents.get_text().strip()\n",
    "\n",
    "                            tmp_content = tmp_content.replace(\"@@imgdesc\", \"<em>\")\n",
    "                            tmp_content = tmp_content.replace(\"@@endimgdesc\", \"</em>\")\n",
    "                            tmp_content = tmp_content.replace(\"@@img\", \"<img src='\")\n",
    "                            tmp_content = tmp_content.replace(\"@@endimg\", \"' />\")\n",
    "                            tmp_content = tmp_content.replace(\"@@strong\", \"<div><strong>\")\n",
    "                            tmp_content = tmp_content.replace(\"@@endstrong\", \"</strong></div>\")\n",
    "                            tmp_content = tmp_content.replace(\"@@br\", \"<br />\")\n",
    "\n",
    "                            detail[\"article_content\"] = tmp_content\n",
    "                            # 원본 링크 (article_url)\n",
    "                            original = header.find(\"a\", {\"class\": \"media_end_head_origin_link\"})['href']\n",
    "                            detail[\"article_url\"] = original\n",
    "\n",
    "                            # 조회수\n",
    "                            detail[\"article_hits\"] = 0\n",
    "\n",
    "                            # 기사 아이디\n",
    "                            last_article_id+=1\n",
    "                            detail[\"article_id\"] = last_article_id\n",
    "\n",
    "                            results.append(detail)\n",
    "                            index+=1\n",
    "                        except HTTPError as e:\n",
    "                            err = e.read()\n",
    "                            code = e.getcode()\n",
    "                            print(code) \n",
    "                page += 1\n",
    "            # 하루 더해서 다음날로 넘어가기\n",
    "            s_date += dt.timedelta(days=1)\n",
    "            print(s_date)\n",
    "        print(\"길이: \",len(results))\n",
    "        if index > 0:\n",
    "            print(subkey,\" 마지막 기사 시간: \", results[len(results)-index]['article_regtime'], \"\\n\")\n",
    "            write_date+=(results[len(results)-index]['article_regtime']+\"\\n\")\n",
    "        else:\n",
    "            print(subkey,\" 마지막 기사 시간: \", last_date, \"\\n\")\n",
    "            write_date+=(last_date+\"\\n\")\n",
    "    # 마지막 기사 시간 저장\n",
    "    writeArticleTime(write_date)\n",
    "        \n",
    "# 결과 출력\n",
    "print(\"전체 길이\", len(results))\n",
    "\n",
    "# 뉴스 데이터 Dataframe 설정\n",
    "news_df = pd.DataFrame(results, columns=column_name)\n",
    "\n",
    "# 인덱스 재정렬\n",
    "news_df = news_df.sort_index(ascending=False)\n",
    "news_df = news_df.reset_index(drop=True)\n",
    "list = news_df[[\"article_id\"]].values.tolist()\n",
    "list.reverse()\n",
    "news_df[[\"article_id\"]]=list\n",
    "\n",
    "# 종료 시간\n",
    "end = time.time()\n",
    "\n",
    "print(f'소요 시간 {end - start}초')\n",
    "\n",
    "# # DB로 저장\n",
    "# db_connection_str = 'mysql+pymysql://root:ssafy@j8a509.p.ssafy.io:3306/ssafy_cow_db'\n",
    "# db_connection = create_engine(db_connection_str)\n",
    "# news_df.to_sql(name='article', con=db_connection, if_exists='append',index=False)  \n",
    "\n",
    "# HDFS 에 데이터 넣기\n",
    "newsdata.put_data(news_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87c391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f01f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
